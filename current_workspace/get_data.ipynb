{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and calculate the data\n",
    "This program is used to plot COVID-19 incidences in Germany by county from March 1, 2020 to the current date (for as long as the API provides the data).\n",
    "<br/><br/>\n",
    "This project contains three files:\n",
    "   - \"get_geographical_data_of_german_counties.ipynb\"\n",
    "   - \"plot_data.ipynb\"\n",
    "   - \"get_data.ipynb\"\n",
    "\n",
    "The file \"get_geographical_data_of_german_counties.ipynb\" saves the following information reachable through the \"AdmUnitID\" (Gemeindeschlüssel) in the dictionary \"counties_geography\":\n",
    "   - name: Name of the county\n",
    "   - population: Number of inhabitants from the last official guess, also used in the official incidence calculations\n",
    "   - geometry: The shape of the county stored in one or more polygons - also determining the position of the county\n",
    "   - raw_geometry: The original version of the geometry used to draw the surroundings of the counties\n",
    "   - area_in_m2: Area of the county in square meters, can not be calculated from the polygons stored in geometry\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The program is called from \"plot_data.ipynb\" or it is run by itself.\n",
    "<br/><br/><br/>\n",
    "The main file is calles \"plot_data.ipynb\" \n",
    "\n",
    "Das main file nennt \n",
    "This project i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The program provides the dictionary \"counties_geography\" with different kinds of geographical information about every German county. For more information check out the file \"get_geographical_data_of_german_counties.ipynb\".\n",
    "<br/><br/>\n",
    "The program saves the accumulated COVID-19 case number of every german county and every day of the pandemic in the dictionary \"covid19\" (reachable by the county's AdmUnitID).\n",
    "<br/><br/>\n",
    "Finally the program calculates the seven days incidence for every county and every day of the pandemic. Additionally it calculates each county's population density and converts the Unix time into UTC.\n",
    "<br/><br/>\n",
    "The data from the API is called \"unpolished\" or \"unmodified\". The process of \"polishing the data\" contains checking the data, cutting away unnecessary data (e.g. object IDs) and for us useless dictionary shells.\n",
    "So that this program provides the slimest version of the COVID-19 data and the geographical data in the end, called \"polished version\" or \"modified data\".<br/><br/>\n",
    "There are three different ways the COVID-19 data and the geographical data are stored:\n",
    "- Stored on the server of the RKI, reachable through the API.\n",
    "- Stored unpolished and unmodified on this machine as backup if the API doesn't work anymore.\n",
    "- Stored polished and ready to go on this machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "Needed to use non-Python functionalities already programmed by someone else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert the API data from json-format into a python list\n",
    "from json2xml.utils import readfromurl\n",
    "import json    # to save the data in \"json\"-format in a file\n",
    "# Used to check if there is a local file with the data or if a new API pull is inevitable\n",
    "import os.path\n",
    "import datetime    # to convert Unix time to UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control\n",
    "Set variables to \"True\" to trigger the action described by the comment and the variable's name.<br/><br/>\n",
    "If multiple of the three variables \"covid19_use_api\", \"covid19_use_api_backup\" and \"covid19_use_polished_data\" are set to \"True\", the last one overwrites all data collected by the others. It is best practice to only set one to \"True\".<br/><br/>\n",
    "If one data source seems to provide faulty data or the necessary files do not exist, the other options are tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The program uses polished geographical data about the counties (True)\n",
    "# or calls get_geographical_data_of_german_counties.ipynb to produce that data (False)\n",
    "counties_geography_use_polished_data = True\n",
    "\n",
    "covid19_use_api = False    # pulls current COVID-19 case numbers from the API\n",
    "covid19_use_api_backup = False    # polishes backup of old API pull\n",
    "covid19_use_polished_data = True    # takes old, already polished data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the controls\n",
    "Check if the necessary files to run the choices made by the controls above exist. Otherwise the data must be taken from somewhere else.<br/>\n",
    "Pulling from the API takes a lot of time and ressources. If the user therefore chooses in the controls above not to pull from the API, this choice should only be changed if it is unavoidable.\n",
    "<br/>\n",
    "There are three ways how data could be missing:\n",
    "- Neither polished nor unpolished data about the German COVID-19 cases are saved on the machine. In this case a new pull from the API is inevitable.\n",
    "- No polished version of the data exists on the machine, but a backup of an old API pull. Therefore the program initiates a pull from the API or a \"pull\" from the backup file with the unpolished data.\n",
    "- No backup of an old API pull exists, but a polished version of the data. If not, the program initiates a pull from the API or uses the polished data. The file with the polished data exists, due to the first condition.\n",
    "\n",
    "\n",
    "<br/>\n",
    "In any case the global control variables are changed accordingly.\n",
    "<br/><br/>\n",
    "The \"number_of_counties\" is also set here: It determines how many counties must be present in the data. If there are less or more, the current data-source is declared a fail and (if possible) another one is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not(os.path.isfile(\"modified_data/german_covid19.txt\")) and \n",
    "    not(os.path.isfile(\"unmodified_data/covid19/dates.txt\"))):    # no files\n",
    "    covid19_use_polished_data = False\n",
    "    covid19_use_api_backup = False\n",
    "    covid19_use_api = True\n",
    "elif not(os.path.isfile(\"modified_data/german_covid19.txt\")):    # no polished version\n",
    "    # and os.path.isfile(\"unmodified_data/covid19/dates.txt\") due to first condition\n",
    "    covid19_use_polished_data = False\n",
    "    # ensuring that one of the other two data sources is used\n",
    "    covid19_use_api_backup = not(covid19_use_api)\n",
    "elif not(os.path.isfile(\"unmodified_data/covid19/dates.txt\")):    # no backup\n",
    "    # and os.path.isfile(\"modified_data/german_covid19.txt\") due to first condition\n",
    "    covid19_use_api_backup = False\n",
    "    # ensuring that one of the others is used\n",
    "    covid19_use_polished_data = not(covid19_use_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_counties = 412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get geographical data of every german county\n",
    "If \"counties_geography_use_polished_data\" is set to True and the needed file exists the polished data from that file is used. <br/>\n",
    "If \"counties_geography_use_polished_data\" is set to False by the user or if the needed file does not exist the file \"get_geographical_data_of_german_counties.ipynb\" gets called to provide new polished data.<br/>\n",
    "For more information where the data comes from and how it gets polished check out the file \"get_geographical_data_of_german_counties.ipynb\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(os.path.isfile(\"modified_data/german_counties_geography.txt\")):\n",
    "    counties_geography_use_polished_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polished county data from file is ready to go!\n"
     ]
    }
   ],
   "source": [
    "if counties_geography_use_polished_data:\n",
    "    with open(\"modified_data/german_counties_geography.txt\", \"r\") as file:\n",
    "        counties_geography = json.loads(file.read())\n",
    "    print(\"Polished county data from file is ready to go!\")\n",
    "else:\n",
    "    no_outputs_from_file_get_shapes_of_german_counties = True\n",
    "    %run get_geographical_data_of_german_counties.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the COVID-19 cases of every german county\n",
    "Saves the COVID-19 cases of every german county since the start of the pandemic in the dictionary \"covid19\" (reachable by the countys AdmUnitID) and the corresponding dates in the dictionary \"non_county_specific_data\".\n",
    "<br/><br/>\n",
    "In the control chapter the user presets what shall be done and the program checks whether the actions are possible or not.<br/>\n",
    "If \"covid19_use_api\" is set True, the program pulls from the [COVID-19 Datenhub provided from the Robert-Koch-Institut](https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/6d78eb3b86ad4466a8e264aa2e32a2e4_0). The data of each county must get pulled separatedly because the API only allows 1000 datapoints at a time and all counties times the number of days is well over 100.000. The identifiers of the counties originate from the keys of the dictionary \"counties_geography\".<br/><br/>\n",
    "The raw data gets checked: If any county has less timestamps than the dates stored in \"non_county_specific_data['unixtime']\", all data gets deleted to prevent use of faulty data and an alternative data-source is chosen.<br/><br/>\n",
    "If the unpolished data passes this rudimentary test it is stored as it is in a txt-file with its AdmUnitID as name in the folder \"covid19\" inside the folder \"unmodified_data\". If any of the folders or any of the files do not yet exist they get created.<br/>\n",
    "This files can be used in further executions as local backup of the API-pull.\n",
    "<br/>\n",
    "<br/>\n",
    "If the use of the data from a local backup of the API-pull is requested and possible, the data is provided without further tests.\n",
    "<br/><br/>\n",
    "The polished version of the data gets stored in the variable \"covid19\" and contains the accumulated cases from every day of the pandemic in a list sorted by the AdmUnitID of the county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url_county(AdmUnitID, True_for_dates_False_for_covid19_cases = False): returns url\n",
    "Used to get the url for the COVID-19 cases of the german county determined by the AdmUnitID.<br/>\n",
    "**AdmUnitId**<br/>\n",
    "-> identifier of the county whichs covid19 cases shall be requested<br/>\n",
    "**True_for_dates_False_for_covid19_cases** (default: False)<br/>\n",
    "-> Determines whether the dates in unixtime or the actual COVID-19 cases shall be requested\n",
    "### find_alternative_source_of_data_and_activate_it(): returns void (modifys multiple global variables)\n",
    "Gets called when the data from a data-source is faulty. Deletes faulty data to prevent use of faulty data. Checks which other data-source could be used and modifys the global variables accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_county(AdmUnitID, True_for_dates_False_for_covid19_cases = False):\n",
    "    url = (\"https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/\" +\n",
    "           \"rki_history_hubv/FeatureServer/0/query?where=AdmUnitId%3D\" +\n",
    "           str(AdmUnitID) + \"&outFields=\")\n",
    "    if True_for_dates_False_for_covid19_cases:\n",
    "        return url + \"Datum&orderByFields=Datum&f=pjson\"\n",
    "    return url + \"KumFall&orderByFields=Datum&f=pjson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alternative_source_of_data_and_activate_it():\n",
    "    global covid19_use_api\n",
    "    global covid19_use_api_backup\n",
    "    global covid19_use_polished_data\n",
    "    global copy_of_covid19_for_debugging_purposes\n",
    "    global covid19\n",
    "    global non_county_specific_data\n",
    "    copy_of_non_county_specific_data_for_debugging_purposes = non_county_specific_data.copy()\n",
    "    copy_of_covid19_for_debugging_purposes = covid19.copy()\n",
    "    del non_county_specific_data    # to prevent accidentall use of faulty data\n",
    "    del covid19    # to prevent accidentall use of faulty data\n",
    "    # check if a local pull of the API exists otherwise use the polished data\n",
    "    if os.path.isfile(\"unmodified_data/covid19/dates.txt\"):\n",
    "        covid19_use_api_backup = True\n",
    "    if os.path.isfile(\"modified_data/german_covid19.txt\"):\n",
    "        covid19_use_polished_data = True\n",
    "    # neither local backup nor polished data found\n",
    "    if not(covid19_use_api_backup) and not(covid19_use_polished_data):\n",
    "        raise Exception(\"No usable data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if new pull from the API is necessary or wished and\n",
    "# if it is even possible otherwise \"pull\" from local backup\n",
    "if covid19_use_api:\n",
    "    print(\"Pulling from API...\")\n",
    "    covid19 = dict()\n",
    "    non_county_specific_data = dict()\n",
    "    # check if the needed directory is availlable - otherwise create it\n",
    "    if not(os.path.isdir(\"unmodified_data/covid19\")): os.makedirs(\"unmodified_data/covid19\")\n",
    "    number_of_timestamps = -1\n",
    "    \n",
    "    # get data - every county must be called individually because of the Max Record Count of the API\n",
    "    for AdmUnitID in list(counties_geography.keys()):\n",
    "        # get dates of first county\n",
    "        if number_of_timestamps == -1:\n",
    "            raw_dates = readfromurl(url_county(AdmUnitID, True))\n",
    "            if len(raw_dates['features']) < 200:\n",
    "                print(\"The dates of {} sends to little timestamps ({}) - check the url\"\n",
    "                      .format(AdmUnitID, len(raw_dates['features'])))\n",
    "                find_alternative_source_of_data_and_activate_it()\n",
    "                covid19_use_api = False\n",
    "                break\n",
    "            number_of_timestamps = len(raw_dates['features'])\n",
    "            non_county_specific_data['unixtime'] = [e['attributes']['Datum'] for e in raw_dates['features']]\n",
    "            # save raw data\n",
    "            with open(\"unmodified_data/covid19/dates.txt\", \"w\") as file:\n",
    "                file.write(json.dumps(raw_dates))\n",
    "\n",
    "        # get countys covid19 data\n",
    "        raw_covid19_data = readfromurl(url_county(AdmUnitID))\n",
    "        if number_of_timestamps != len(raw_covid19_data['features']):\n",
    "            print(\"The provided data from the API does not have the same number of timestamps of \" +\n",
    "                  \"{}, it has {}.\".format(number_of_timestamps, len(raw_covid19_data['features'])))\n",
    "            find_alternative_source_of_data_and_activate_it()\n",
    "            covid19_use_api = False\n",
    "            break\n",
    "        covid19[AdmUnitID] = dict()\n",
    "        covid19[AdmUnitID]['cases'] = [e['attributes']['KumFall'] for e in raw_covid19_data['features']]\n",
    "        with open(\"unmodified_data/covid19/\" + AdmUnitID + \".txt\", \"w\") as file:\n",
    "            file.write(json.dumps(raw_covid19_data))\n",
    "        \n",
    "    if covid19_use_api:\n",
    "        covid19_use_polished_data = False\n",
    "        covid19_use_api_backup = False\n",
    "        print(\"Covid19 Data directly from API is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Pull\" from local API backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data from local backup originating from old API pull\n",
    "# covid19_use_api could be modified in the if-statement - therefore no else-statement here\n",
    "if not(covid19_use_api) and covid19_use_api_backup:\n",
    "    print(\"Reading backup of old API pull...\")\n",
    "    covid19 = dict()\n",
    "    non_county_specific_data = dict()\n",
    "    list_of_countys = list(counties_geography.keys())\n",
    "    # get the dates\n",
    "    with open(\"unmodified_data/covid19/dates.txt\", \"r\") as file:\n",
    "        raw_dates = json.loads(file.read())\n",
    "    non_county_specific_data['unixtime'] = [e['attributes']['Datum'] for e in raw_dates['features']]\n",
    "    number_of_timestamps = len(non_county_specific_data['unixtime'])\n",
    "\n",
    "    for root, dirs, files in os.walk('unmodified_data/covid19'):\n",
    "        # to little dates - something is wrong. Checking here to skip for-loop\n",
    "        if len(raw_dates['features']) < 200:\n",
    "            print(\"There are only {} dates - check your backup or make a new pull from the api.\"\n",
    "                  .format(len(raw_dates['features'])))\n",
    "            find_alternative_source_of_data_and_activate_it()\n",
    "            covid19_use_api = False\n",
    "            break\n",
    "        for filename in files:\n",
    "            AdmUnitID = filename[:-4]\n",
    "            if AdmUnitID == 'dates':    # already done\n",
    "                continue\n",
    "\n",
    "            list_of_countys.remove(AdmUnitID)\n",
    "            covid19[AdmUnitID] = dict()\n",
    "            with open(os.path.join(root, filename), \"r\") as file:\n",
    "                covid19[AdmUnitID]['cases'] = [e['attributes']['KumFall'] for e in json.loads(file.read())['features']]\n",
    "\n",
    "            if number_of_timestamps != len(covid19[AdmUnitID]['cases']):\n",
    "                print(\"The data from file {} does not have {} timestamps, it has {}.\"\n",
    "                      .format(filename, number_of_timestamps, len(covid19[AdmUnitID])))\n",
    "                find_alternative_source_of_data_and_activate_it()\n",
    "                covid19_use_api_backup = False\n",
    "                break\n",
    "\n",
    "    if len(list_of_countys) > 0 and covid19_use_api_backup:\n",
    "        print(\"No backup found for {}\".format(list_of_countys))\n",
    "        find_alternative_source_of_data_and_activate_it()\n",
    "        covid19_use_api_backup = False\n",
    "\n",
    "    if covid19_use_api_backup:\n",
    "        covid19_use_polished_data = False\n",
    "        covid19_use_api = False\n",
    "        print(\"Covid19 Data from (maybe old) API-pull-backup is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get names of the german federal states\n",
    "This data is hardcoded because it is unlikely to change. Even if the names of the federal states get outdatet and don't fit the current official name the functionality of this project will not be affected.\n",
    "The names originate from  the [COVID-19 Datenhub provided from the Robert-Koch-Institut](https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/rki_admunit_hubv/FeatureServer/0/query?where=AdmUnitId%3C20&resultType=none&outFields=*&f=pjson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    non_county_specific_data['states'] = {\n",
    "        \"1\" : \"Schleswig-Holstein\",\n",
    "        \"2\" : \"Hamburg\",\n",
    "        \"3\" : \"Niedersachsen\",\n",
    "        \"4\" : \"Bremen\",\n",
    "        \"5\" : \"Nordrhein-Westfalen\",\n",
    "        \"6\" : \"Hessen\",\n",
    "        \"7\" : \"Rheinland-Pfalz\",\n",
    "        \"8\" : \"Baden-Württemberg\",\n",
    "        \"9\" : \"Bayern\",\n",
    "        \"10\" : \"Saarland\",\n",
    "        \"11\" : \"Berlin\",\n",
    "        \"12\" : \"Brandenburg\",\n",
    "        \"13\" : \"Mecklenburg-Vorpommern\",\n",
    "        \"14\" : \"Sachsen\",\n",
    "        \"15\" : \"Sachsen-Anhalt\",\n",
    "        \"16\" : \"Thüringen\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate seven days incidence and\tpopulation density\n",
    "The incidence is calculated from the number of cases seven days prior (set to zero if not defined), the cases of the current day (both from \"covid19[AdmUnitID]['cases']\") and the number of inhabitants (\"counties_geography[AdmUnitID]['population']\").<br/>\n",
    "To get all new cases in that county within the last seven days the program subtracts the accumulated cases seven days earlier from the accumulated cases of the current day. Afterwards this number of cases is divided by the counties population. To scale it to 100.000 inhabitants, the result is multiplied by 100.000.<br/>\n",
    "This is done for every case number of every county.<br/><br/>\n",
    "The population density is calculated by dividing the population number by the area in square meters. To scale it to kilometers, the result is multiplied by 1000000.\n",
    "<br/><br/>\n",
    "The highest and lowest seven days incidence, the highest and lowest case number and the highest and lowest population density are stored in the dictionary \"non_county_specific_data\" to have a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    non_county_specific_data['highest_case_number'] = 0\n",
    "    non_county_specific_data['lowest_case_number'] = 100000000000000\n",
    "    non_county_specific_data['highest_incidence'] = 0\n",
    "    non_county_specific_data['lowest_incidence'] = 100000000000000\n",
    "    for AdmUnitID in covid19.keys():\n",
    "        covid19[AdmUnitID]['incidences'] = list()\n",
    "        for timestamp in range(len(covid19[AdmUnitID]['cases'])):\n",
    "            cases_7_days_prior = 0\n",
    "            cases_on_day = covid19[AdmUnitID]['cases'][timestamp]\n",
    "            if timestamp >= 7:\n",
    "                cases_7_days_prior = covid19[AdmUnitID]['cases'][timestamp - 7]\n",
    "            incidence = (((cases_on_day - cases_7_days_prior) * 100000) /\n",
    "                         counties_geography[AdmUnitID]['population'])\n",
    "            covid19[AdmUnitID]['incidences'].append(incidence)\n",
    "            if non_county_specific_data['highest_case_number'] < cases_on_day:\n",
    "                non_county_specific_data['highest_case_number'] = cases_on_day\n",
    "            if non_county_specific_data['lowest_case_number'] > cases_on_day:\n",
    "                non_county_specific_data['lowest_case_number'] = cases_on_day\n",
    "            if non_county_specific_data['highest_incidence'] < incidence:\n",
    "                non_county_specific_data['highest_incidence'] = incidence\n",
    "            if non_county_specific_data['lowest_incidence'] > incidence:\n",
    "                non_county_specific_data['lowest_incidence'] = incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    # is calculated here instead inside the get_shapes_of_german_counties.ipynb-file\n",
    "    # to be able to put it together in one dictionary non_county_specific_data\n",
    "    non_county_specific_data['highest_population_density'] = 0\n",
    "    non_county_specific_data['lowest_population_density'] = 100000000000000\n",
    "    for county in counties_geography.values():\n",
    "        county[\"population_density\"] = (county['population'] * 1000000)/county['area_in_m2']\n",
    "        if non_county_specific_data['highest_population_density'] < county[\"population_density\"]:\n",
    "            non_county_specific_data['highest_population_density'] = county[\"population_density\"]\n",
    "        if non_county_specific_data['lowest_population_density'] > county[\"population_density\"]:\n",
    "            non_county_specific_data['lowest_population_density'] = county[\"population_density\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and save the polished covid19 data\n",
    "Before the COVID-19 cases get saved into the file \"german_covid19.txt\" inside the folder \"modified_data\" they get checked once again to ensure that during the handling nothing got lost or changed.\n",
    "<br/>\n",
    "It is checked if there are less or more counties than defined in the variable number_of_counties and if every list of cases is as long as the dedicated dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    covid19_data_seems_to_be_flawless = True    # Assume everything is correct\n",
    "    if len(covid19) != number_of_counties:\n",
    "        print(\"covid19 has not the right amount of counties: {} instead of {}.\"\n",
    "                .format(len(covid19), number_of_counties))\n",
    "        covid19_data_seems_to_be_flawless = False\n",
    "    for AdmUnitID in covid19.keys():\n",
    "        if len(covid19[AdmUnitID]['cases']) != len(non_county_specific_data['unixtime']):\n",
    "            print(\"The county {} has not the right amount of dates: {} instead of {}.\"\n",
    "                    .format(county, len(covid19[AdmUnitID]['cases']),\n",
    "                            len(non_county_specific_data['unixtime'])))\n",
    "            covid19_data_seems_to_be_flawless = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data) and covid19_data_seems_to_be_flawless:\n",
    "    # check if the needed directory is availlable - otherwise create it\n",
    "    if not(os.path.isdir(\"modified_data\")): os.makedirs(\"modified_data\")\n",
    "    with open(\"modified_data/german_covid19.txt\", \"w\") as file:\n",
    "        file.write(json.dumps((covid19, non_county_specific_data)))\n",
    "    print(\"Saved seemingly flawless covid19 data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the polished data\n",
    "If the pull from the API and/or the \"pull\" from the local backup failed or the user chose to use the polished data, the file \"german_covid19.txt\" inside the folder \"modified_data\" gets opened and the data stored in the variables \"covid19\" and \"non_county_specific_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polished covid19 data from file is ready to go!\n"
     ]
    }
   ],
   "source": [
    "if covid19_use_polished_data:\n",
    "    covid19_use_api_backup = False\n",
    "    covid19_use_api = False\n",
    "    with open(\"modified_data/german_covid19.txt\", \"r\") as file:\n",
    "        covid19, non_county_specific_data = json.loads(file.read())\n",
    "    print(\"Polished covid19 data from file is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add UTC time\n",
    "Humans are in general not used to the Unix time that is why the more present kind of time format UTC is chosen. The exact hour in Germany and the time shift is not taken to account, because the data is only compared to other data with the same time shift.\n",
    "<br/><br/>\n",
    "The UTC time is added after saving the data, because the UTC time format can not be saved in json format. Therefore it must always be done. Calculating it inside the file \"get_data.ipynb\" keeps the plotting of the data strictly separated from the pulling and modifying of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_county_specific_data['UTC'] = [datetime.datetime.utcfromtimestamp(date/1000)\n",
    "                           for date in non_county_specific_data['unixtime']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
