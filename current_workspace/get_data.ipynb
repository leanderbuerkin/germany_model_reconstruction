{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json2xml.utils import readfromurl    # to convert the json from the API into a python list\n",
    "import json    # to save the data as json in a file\n",
    "import os.path    # to check if their's a local file with the API-data or if a new API pull is inevitable\n",
    "import datetime    # to convert linux time to readable time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls new data about the shapes of the counties from the API \n",
    "# (or takes old, local backup from API, depends on the variable counties_geography_new_pull_from_api)\n",
    "# and polishes it again\n",
    "counties_geography_use_polished_data = True\n",
    "# pulls new data about the covid 19 cases of the counties from the API \n",
    "# (or takes old, local backup from API, depends on the variable covid19_use_api_backup)\n",
    "# and polishes it again\n",
    "covid19_use_api = False\n",
    "covid19_use_api_backup = False\n",
    "# takes old, already polished data\n",
    "covid19_use_polished_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a polished version of the german counties covid19 cases exists\n",
    "# check if a local pull of the API exists otherwise initiate a new pull from the API\n",
    "if (not(os.path.isfile(\"modified_data/german_covid19.txt\")) and \n",
    "    not(os.path.isfile(\"unmodified_data/covid19/dates.txt\"))):    # no files\n",
    "    covid19_use_polished_data = False\n",
    "    covid19_use_api = True\n",
    "elif not(os.path.isfile(\"modified_data/german_covid19.txt\")):    # no polished\n",
    "    covid19_use_polished_data = False\n",
    "    # ensuring that one of the others is used\n",
    "    covid19_use_api_backup = not(covid19_use_api)\n",
    "elif not(os.path.isfile(\"unmodified_data/covid19/dates.txt\")):    # no backup\n",
    "    covid19_use_api_backup = False\n",
    "    # ensuring that one of the others is used\n",
    "    covid19_use_polished_data = not(covid19_use_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data does not contain that many counties,\n",
    "# the program raises an error and pulls from a local backup of the API pull\n",
    "number_of_counties = 412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get county shapes of the german counties\n",
    "Either use old polished version or make new pull from API and polish it.\n",
    "Depending on the variable counties_geography_use_polished_data in the controls of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a polished version of the german counties shapes exists\n",
    "# and otherwise initiate a new pull from the API\n",
    "if not(os.path.isfile(\"modified_data/german_counties_geography.txt\")):\n",
    "    counties_geography_use_polished_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directly from API is ready to go!\n",
      "Data from (maybe old) API-pull is ready to go!\n"
     ]
    }
   ],
   "source": [
    "if counties_geography_use_polished_data:\n",
    "    with open(\"modified_data/german_counties_geography.txt\", \"r\") as file:\n",
    "        counties_geography = json.loads(file.read())\n",
    "    print(\"Polished county data from file is ready to go!\")\n",
    "else:\n",
    "    no_outputs_from_file_get_shapes_of_german_counties = True\n",
    "    %run get_shapes_of_german_counties.ipynb\n",
    "    print(\"Data from (maybe old) API-pull is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get covid19 data of the german counties\n",
    "Either use old polished version or make new pull from API and polish it.\n",
    "Depending on the variable covid19_use_polished_data in the controls of this file.\n",
    "<br/>\n",
    "<br/>\n",
    "Get the correct URLs to the arcgis server:\n",
    "We must pull all counties separatedly because the API only allows 1000 datapoints at a time.\n",
    "<br/>\n",
    "<br/>\n",
    "This part is not outsourced because it is shorter than the pulling and polishing of the shape data and to ensure that the shape data is availlable to compare length and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_county(AdmUnitID, True_for_dates_False_for_covid19_cases):\n",
    "    url = (\"https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/\" +\n",
    "           \"rki_history_hubv/FeatureServer/0/query?where=AdmUnitId%3D\" +\n",
    "           str(AdmUnitID) + \"&outFields=\")\n",
    "    if True_for_dates_False_for_covid19_cases:\n",
    "        return url + \"Datum&orderByFields=Datum&f=pjson\"\n",
    "    return url + \"KumFall&orderByFields=Datum&f=pjson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alternative_source_of_data_and_activate_it():\n",
    "    global covid19_use_api\n",
    "    global covid19_use_api_backup\n",
    "    global covid19_use_polished_data\n",
    "    global copy_of_covid19_for_debugging_purposes\n",
    "    global covid19\n",
    "    global non_county_specific_data\n",
    "    copy_of_non_county_specific_data_for_debugging_purposes = non_county_specific_data.copy()\n",
    "    copy_of_covid19_for_debugging_purposes = covid19.copy()\n",
    "    del non_county_specific_data    # to prevent accidentall use of faulty data\n",
    "    del covid19    # to prevent accidentall use of faulty data\n",
    "    # check if a local pull of the API exists otherwise use the polished data\n",
    "    if os.path.isfile(\"unmodified_data/covid19/dates.txt\"):\n",
    "        covid19_use_api_backup = True\n",
    "    if os.path.isfile(\"modified_data/german_covid19.txt\"):\n",
    "        covid19_use_polished_data = True\n",
    "    # neither local backup nor polished data found\n",
    "    if not(covid19_use_api_backup) and not(covid19_use_polished_data):\n",
    "        raise Exception(\"No usable data found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling from API...\n",
      "Covid19 Data directly from API is ready to go!\n"
     ]
    }
   ],
   "source": [
    "# check if new pull from the API is necessary or wished and\n",
    "# if it is even possible otherwise \"pull\" from local backup\n",
    "if covid19_use_api:\n",
    "    print(\"Pulling from API...\")\n",
    "    covid19 = dict()\n",
    "    non_county_specific_data = dict()\n",
    "    # check if the needed directory is availlable - otherwise create it\n",
    "    if not(os.path.isdir(\"unmodified_data/covid19\")): os.makedirs(\"unmodified_data/covid19\")\n",
    "    number_of_timestamps = -1\n",
    "    \n",
    "    # get data - every county must be called individually because of the Max Record Count of the API\n",
    "    for AdmUnitID in list(counties_geography.keys()):\n",
    "        # get dates of first county\n",
    "        if number_of_timestamps == -1:\n",
    "            raw_dates = readfromurl(url_county(AdmUnitID, True))\n",
    "            if len(raw_dates['features']) < 200:\n",
    "                print(\"The dates of {} sends to little timestamps ({}) - check the url\"\n",
    "                      .format(AdmUnitID, len(raw_dates['features'])))\n",
    "                find_alternative_source_of_data_and_activate_it()\n",
    "                covid19_use_api = False\n",
    "                break\n",
    "            number_of_timestamps = len(raw_dates['features'])\n",
    "            non_county_specific_data['unixtime'] = [e['attributes']['Datum'] for e in raw_dates['features']]\n",
    "            # save raw data\n",
    "            with open(\"unmodified_data/covid19/dates.txt\", \"w\") as file:\n",
    "                file.write(json.dumps(raw_dates))\n",
    "\n",
    "        # get countys covid19 data\n",
    "        raw_covid19_data = readfromurl(url_county(AdmUnitID, False))\n",
    "        if number_of_timestamps != len(raw_covid19_data['features']):\n",
    "            print(\"The provided data from the API does not have the same number of timestamps of \" +\n",
    "                  \"{}, it has {}.\".format(number_of_timestamps, len(raw_covid19_data['features'])))\n",
    "            find_alternative_source_of_data_and_activate_it()\n",
    "            covid19_use_api = False\n",
    "            break\n",
    "        covid19[AdmUnitID] = dict()\n",
    "        covid19[AdmUnitID]['cases'] = [e['attributes']['KumFall'] for e in raw_covid19_data['features']]\n",
    "        with open(\"unmodified_data/covid19/\" + AdmUnitID + \".txt\", \"w\") as file:\n",
    "            file.write(json.dumps(raw_covid19_data))\n",
    "        \n",
    "    if covid19_use_api:\n",
    "        print(\"Covid19 Data directly from API is ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data from local backup originating from old API pull\n",
    "# covid19_use_api could be modified in the if-statement - therefore no else-statement here\n",
    "if not(covid19_use_api) and covid19_use_api_backup:\n",
    "    print(\"Reading backup of old API pull...\")\n",
    "    covid19 = dict()\n",
    "    non_county_specific_data = dict()\n",
    "    list_of_countys = list(counties_geography.keys())\n",
    "    # get the dates\n",
    "    with open(\"unmodified_data/covid19/dates.txt\", \"r\") as file:\n",
    "        raw_dates = json.loads(file.read())\n",
    "    non_county_specific_data['unixtime'] = [e['attributes']['Datum'] for e in raw_dates['features']]\n",
    "    number_of_timestamps = len(non_county_specific_data['unixtime'])\n",
    "\n",
    "    for root, dirs, files in os.walk('unmodified_data/covid19'):\n",
    "        # to little dates - something is wrong. Checking here to skip for-loop\n",
    "        if len(raw_dates['features']) < 200:\n",
    "            print(\"There are only {} dates - check your backup or make a new pull from the api.\"\n",
    "                  .format(len(raw_dates['features'])))\n",
    "            find_alternative_source_of_data_and_activate_it()\n",
    "            covid19_use_api = False\n",
    "            break\n",
    "        for filename in files:\n",
    "            AdmUnitID = filename[:-4]\n",
    "            if AdmUnitID == 'dates':    # already done\n",
    "                continue\n",
    "\n",
    "            list_of_countys.remove(AdmUnitID)\n",
    "            covid19[AdmUnitID] = dict()\n",
    "            with open(os.path.join(root, filename), \"r\") as file:\n",
    "                covid19[AdmUnitID]['cases'] = [e['attributes']['KumFall'] for e in json.loads(file.read())['features']]\n",
    "\n",
    "            if number_of_timestamps != len(covid19[AdmUnitID]['cases']):\n",
    "                print(\"The data from file {} does not have {} timestamps, it has {}.\"\n",
    "                      .format(filename, number_of_timestamps, len(covid19[AdmUnitID])))\n",
    "                find_alternative_source_of_data_and_activate_it()\n",
    "                covid19_use_api_backup = False\n",
    "                break\n",
    "\n",
    "    if len(list_of_countys) > 0 and covid19_use_api_backup:\n",
    "        print(\"No backup found for {}\".format(list_of_countys))\n",
    "        find_alternative_source_of_data_and_activate_it()\n",
    "        covid19_use_api_backup = False\n",
    "\n",
    "    if covid19_use_api_backup:\n",
    "        print(\"Covid19 Data from (maybe old) API-pull-backup is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get AdmUnitID of the german federal states\n",
    "This data is harcoded because it is unlikely to change. Even if the names of the federal states get outdatet and don't fit the current official name the functionality of this project will not be affected.\n",
    "The names originate from arcgis.\n",
    "https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/rki_admunit_hubv/FeatureServer/0/query?where=AdmUnitId%3C20&resultType=none&outFields=*&f=pjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    non_county_specific_data['states'] = {\n",
    "        \"1\":\"Schleswig-Holstein\",\n",
    "        \"2\":\"Hamburg\",\n",
    "        \"3\" : \"Niedersachsen\",\n",
    "        \"4\" : \"Bremen\",\n",
    "        \"5\" : \"Nordrhein-Westfalen\",\n",
    "        \"6\" : \"Hessen\",\n",
    "        \"7\" : \"Rheinland-Pfalz\",\n",
    "        \"8\" : \"Baden-Württemberg\",\n",
    "        \"9\" : \"Bayern\",\n",
    "        \"10\" : \"Saarland\",\n",
    "        \"11\" :  \"Berlin\",\n",
    "        \"12\" : \"Brandenburg\",\n",
    "        \"13\" : \"Mecklenburg-Vorpommern\",\n",
    "        \"14\" : \"Sachsen\",\n",
    "        \"15\" : \"Sachsen-Anhalt\",\n",
    "        \"16\" : \"Thüringen\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate seven days incidence and\tpopulation density and find highest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    non_county_specific_data['highest_case_number'] = 0\n",
    "    non_county_specific_data['lowest_case_number'] = 100000000000000\n",
    "    non_county_specific_data['highest_incidence'] = 0\n",
    "    non_county_specific_data['lowest_incidence'] = 100000000000000\n",
    "    for AdmUnitID in covid19.keys():\n",
    "        covid19[AdmUnitID]['incidences'] = list()\n",
    "        for timestamp in range(len(covid19[AdmUnitID]['cases'])):\n",
    "            cases_7_days_prior = 0\n",
    "            cases_on_day = covid19[AdmUnitID]['cases'][timestamp]\n",
    "            if timestamp >= 7:\n",
    "                cases_7_days_prior = covid19[AdmUnitID]['cases'][timestamp - 7]\n",
    "            incidence = (((cases_on_day - cases_7_days_prior) * 100000) /\n",
    "                         counties_geography[AdmUnitID]['population'])\n",
    "            covid19[AdmUnitID]['incidences'].append(incidence)\n",
    "            if non_county_specific_data['highest_case_number'] < cases_on_day:\n",
    "                non_county_specific_data['highest_case_number'] = cases_on_day\n",
    "            if non_county_specific_data['lowest_case_number'] > cases_on_day:\n",
    "                non_county_specific_data['lowest_case_number'] = cases_on_day\n",
    "            if non_county_specific_data['highest_incidence'] < incidence:\n",
    "                non_county_specific_data['highest_incidence'] = incidence\n",
    "            if non_county_specific_data['lowest_incidence'] > incidence:\n",
    "                non_county_specific_data['lowest_incidence'] = incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    # is calculated here instead inside the get_shapes_of_german_counties.ipynb-file\n",
    "    # to be able to put it together in one dictionary non_county_specific_data\n",
    "    non_county_specific_data['highest_population_density'] = 0\n",
    "    non_county_specific_data['lowest_population_density'] = 100000000000000\n",
    "    for county in counties_geography.values():\n",
    "        county[\"population_density\"] = (county['population'] * 1000000)/county['area_in_m2']\n",
    "        if non_county_specific_data['highest_population_density'] < county[\"population_density\"]:\n",
    "            non_county_specific_data['highest_population_density'] = county[\"population_density\"]\n",
    "        if non_county_specific_data['lowest_population_density'] > county[\"population_density\"]:\n",
    "            non_county_specific_data['lowest_population_density'] = county[\"population_density\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the polished data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if covid19_use_polished_data:\n",
    "    with open(\"modified_data/german_covid19.txt\", \"r\") as file:\n",
    "        covid19, non_county_specific_data = json.loads(file.read())\n",
    "    print(\"Polished covid19 data from file is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save polished covid19 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_polished_data():\n",
    "    result = True    # Assume everything is correct\n",
    "    if len(covid19) != number_of_counties:\n",
    "        print(\"covid19 has not the right amount of counties: {} instead of {}.\"\n",
    "              .format(len(covid19), number_of_counties))\n",
    "        result = False\n",
    "    for AdmUnitID in covid19.keys():\n",
    "        if len(covid19[AdmUnitID]['cases']) != len(non_county_specific_data['unixtime']):\n",
    "            print(\"The county {} has not the right amount of dates: {} instead of {}.\"\n",
    "                  .format(county, len(covid19[AdmUnitID]['cases']),\n",
    "                          len(non_county_specific_data['unixtime'])))\n",
    "            result = False\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved seemingly flawless covid19 data.\n"
     ]
    }
   ],
   "source": [
    "if check_polished_data():\n",
    "    # check if the needed directory is availlable - otherwise create it\n",
    "    if not(os.path.isdir(\"modified_data\")): os.makedirs(\"modified_data\")\n",
    "    with open(\"modified_data/german_covid19.txt\", \"w\") as file:\n",
    "        file.write(json.dumps((covid19, non_county_specific_data)))\n",
    "    print(\"Saved seemingly flawless covid19 data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add human-readable time\n",
    "Is added inhere because it can't be safed. Calculating it inhere keeps the plotting part very clean and excludes all data manipulation from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_county_specific_data['UTC'] = [datetime.datetime.utcfromtimestamp(date/1000)\n",
    "                           for date in non_county_specific_data['unixtime']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
