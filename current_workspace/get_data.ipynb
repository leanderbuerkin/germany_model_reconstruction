{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and calculate the data\n",
    "Read the file [\"Readme.ipynb\"](Readme.ipynb) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "Needed to use non-Python functionalities already programmed by someone else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to convert the API data from json-format into a Python list\n",
    "from json2xml.utils import readfromurl\n",
    "import json    # to save the data in \"json\"-format in a file\n",
    "# Used to check if there is a local file with the data or if a new API pull is inevitable\n",
    "import os.path\n",
    "import datetime    # to convert Unix time to UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control\n",
    "Set variables to \"True\" to trigger the action described by the comment and the variable's name.<br/><br/>\n",
    "If multiple of the three variables \"covid19_use_api\", \"covid19_use_api_backup\" and \"covid19_use_polished_data\" are set to \"True\", the last one overwrites all data collected by the others. It is best practice to only set one variable to \"True\".<br/><br/>\n",
    "If one data source seems to provide faulty data or the necessary files do not exist, try out the other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The program uses polished geographical data about the counties (True)\n",
    "# or calls get_geographical_data_of_german_counties.ipynb to produce that data (False)\n",
    "counties_geography_use_polished_data = True\n",
    "\n",
    "covid19_use_api = False    # pulls current COVID-19 case numbers from the API\n",
    "covid19_use_api_backup = False    # polishes backup of old API pull\n",
    "covid19_use_polished_data = True    # takes old, already polished data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Controls\n",
    "Check if the necessary files to run the choices made by the controls above exist. Otherwise the data must be taken from somewhere else.<br/>\n",
    "Pulling from the API takes a lot of time and ressources. If the user therefore chooses in the controls above not to pull from the API, this choice should only be changed if it is unavoidable.\n",
    "<br/><br/>\n",
    "There are three ways how data could be missing:\n",
    "- Neither polished nor unpolished data about the German COVID-19 cases are saved on the machine. In this case a new pull from the API is inevitable.\n",
    "- No polished version of the data exists on the machine, but a backup of an old API pull does. Therefore the program initiates a pull from the API or a \"pull\" from the backup file with the unpolished data.\n",
    "- No backup of an old API pull exists, but a polished version of the data does. If not, the program initiates a pull from the API or uses the polished data. The file with the polished data exists due to the first condition.\n",
    "\n",
    "In each respective case the global control variables are changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not(os.path.isfile(\"polished_data/german_covid19.txt\")) and \n",
    "    not(os.path.isfile(\"unpolished_data/covid19/dates.txt\"))):    # no files\n",
    "    covid19_use_polished_data = False\n",
    "    covid19_use_api_backup = False\n",
    "    covid19_use_api = True\n",
    "elif not(os.path.isfile(\"polished_data/german_covid19.txt\")):    # no polished version\n",
    "    # and os.path.isfile(\"unpolished_data/covid19/dates.txt\") due to first condition\n",
    "    covid19_use_polished_data = False\n",
    "    # ensuring that one of the other two data sources is used\n",
    "    covid19_use_api_backup = not(covid19_use_api)\n",
    "elif not(os.path.isfile(\"unpolished_data/covid19/dates.txt\")):    # no backup\n",
    "    # and os.path.isfile(\"polished_data/german_covid19.txt\") due to first condition\n",
    "    covid19_use_api_backup = False\n",
    "    # ensuring that one of the others is used\n",
    "    covid19_use_polished_data = not(covid19_use_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"number_of_counties\" is also set here: It determines how many counties must be present in the data. If there are fewer or more, the current data source is declared a fail and (if possible) another one is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_counties = 412"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Geographical Data of Every German County\n",
    "If \"counties_geography_use_polished_data\" is set to \"True\" and the required file exists, the polished data from that file is used. <br/>\n",
    "If \"counties_geography_use_polished_data\" is set to \"False\" by the user or if the required file does not exist, the file \"get_geographical_data_of_german_counties.ipynb\" is called to provide new polished data.<br/>\n",
    "For more information on where the data comes from and how it is polished check out the file \"get_geographical_data_of_german_counties.ipynb\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(os.path.isfile(\"polished_data/german_counties_geography.txt\")):\n",
    "    counties_geography_use_polished_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directly from API is ready to go!\n"
     ]
    }
   ],
   "source": [
    "if counties_geography_use_polished_data:\n",
    "    with open(\"polished_data/german_counties_geography.txt\", \"r\") as file:\n",
    "        counties_geography = json.loads(file.read())\n",
    "    print(\"Polished county data from file is ready to go!\")\n",
    "else:\n",
    "    no_outputs_from_file_get_shapes_of_german_counties = True\n",
    "    %run get_geographical_data_of_german_counties.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the COVID-19 Cases of Every German County\n",
    "Saves the COVID-19 cases of every German county since the start of the pandemic in the dictionary \"covid19\" (reachable by the countys AdmUnitID) and the corresponding dates in the dictionary \"non_county_specific_data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "**url_county(AdmUnitID, True_for_dates_False_for_covid19_cases = False)**: returns url<br/>\n",
    "Used to get the url for the COVID-19 cases of the German county determined by the AdmUnitID.<br/>\n",
    "*AdmUnitId*<br/>\n",
    "-> identifier of the county whichs covid19 cases should be requested<br/>\n",
    "*True_for_dates_False_for_covid19_cases* (default: False)<br/>\n",
    "-> Determines whether the dates in Unix time format or the actual COVID-19 cases should be requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_county(AdmUnitID, True_for_dates_False_for_covid19_cases = False):\n",
    "    url = (\"https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/\" +\n",
    "           \"rki_history_hubv/FeatureServer/0/query?where=AdmUnitId%3D\" +\n",
    "           str(AdmUnitID) + \"&outFields=\")\n",
    "    if True_for_dates_False_for_covid19_cases:\n",
    "        return url + \"Datum&orderByFields=Datum&f=pjson\"\n",
    "    return url + \"KumFall&orderByFields=Datum&f=pjson\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find_alternative_source_of_data_and_activate_it()**: returns void (modifys multiple global variables)<br/>\n",
    "Gets called when the data from a data source is faulty. Deletes faulty data to prevent use of faulty data. Checks which other data source could be used and modifies the global variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alternative_source_of_data_and_activate_it():\n",
    "    global covid19_use_api\n",
    "    global covid19_use_api_backup\n",
    "    global covid19_use_polished_data\n",
    "    global copy_of_covid19_for_debugging_purposes\n",
    "    global covid19\n",
    "    global non_county_specific_data\n",
    "    copy_of_non_county_specific_data_for_debugging_purposes = non_county_specific_data.copy()\n",
    "    copy_of_covid19_for_debugging_purposes = covid19.copy()\n",
    "    del non_county_specific_data    # to prevent accidental use of faulty data\n",
    "    del covid19    # to prevent accidental use of faulty data\n",
    "    # check if a local pull of the API exists otherwise use the polished data\n",
    "    if os.path.isfile(\"unpolished_data/covid19/dates.txt\"):\n",
    "        covid19_use_api_backup = True\n",
    "    elif os.path.isfile(\"polished_data/german_covid19.txt\"):\n",
    "        covid19_use_polished_data = True\n",
    "    # neither local backup nor polished data found\n",
    "    if not(covid19_use_api_backup) and not(covid19_use_polished_data):\n",
    "        raise Exception(\"No usable data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull from API\n",
    "If \"covid19_use_api\" is set to \"True\", the program pulls from the [\"COVID-19 Datenhub\"](https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/6d78eb3b86ad4466a8e264aa2e32a2e4_0). The data of each county must be pulled separatedly because the API only allows for 1,000 datapoints at a time and all counties times the number of days is well over 100,000. The identifiers of the counties originate from the keys of the dictionary \"counties_geography\".<br/><br/>\n",
    "First, the received data is checked: If any county has fewer timestamps than the dates stored in \"non_county_specific_data['unixtime']\", all data gets deleted to prevent the use of faulty data and an alternative data source is chosen.<br/><br/>\n",
    "If the unpolished data passes this rudimentary test, it is stored as it is in a \".txt-file\" with its AdmUnitID as its name in the folder \"covid19\" inside the folder \"unpolished_data\". If any of the folders or any of the files do not yet exist, they are created.<br/>\n",
    "This file can be used in further executions as local backup of the API-pull.\n",
    "<br/><br/>\n",
    "At the end of this chapter the polished version of the data is stored in the dictionary \"covid19\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling from API...\n",
      "Covid19 Data directly from API is ready to go!\n"
     ]
    }
   ],
   "source": [
    "if covid19_use_api:\n",
    "    print(\"Pulling from API...\")\n",
    "    covid19 = dict()\n",
    "    non_county_specific_data = dict()\n",
    "    # check if the needed directory is available - otherwise create it\n",
    "    if not(os.path.isdir(\"unpolished_data/covid19\")): os.makedirs(\"unpolished_data/covid19\")\n",
    "    number_of_timestamps = -1\n",
    "    \n",
    "    for AdmUnitID in list(counties_geography.keys()):\n",
    "        # get dates of first county\n",
    "        if number_of_timestamps == -1:\n",
    "            raw_dates = readfromurl(url_county(AdmUnitID, True))\n",
    "            if len(raw_dates['features']) < 200:\n",
    "                print(\"The dates of {} sends to little timestamps ({}) - check the url\"\n",
    "                      .format(AdmUnitID, len(raw_dates['features'])))\n",
    "                find_alternative_source_of_data_and_activate_it()\n",
    "                covid19_use_api = False\n",
    "                break\n",
    "            number_of_timestamps = len(raw_dates['features'])\n",
    "            non_county_specific_data['unixtime'] = [e['attributes']['Datum'] for e in raw_dates['features']]\n",
    "            # save raw data\n",
    "            with open(\"unpolished_data/covid19/dates.txt\", \"w\") as file:\n",
    "                file.write(json.dumps(raw_dates))\n",
    "\n",
    "        # get countys covid19 data\n",
    "        raw_covid19_data = readfromurl(url_county(AdmUnitID))\n",
    "        if number_of_timestamps != len(raw_covid19_data['features']):\n",
    "            print(\"The provided data from the API does not have the same number of timestamps of \" +\n",
    "                  \"{}, it has {}.\".format(number_of_timestamps, len(raw_covid19_data['features'])))\n",
    "            find_alternative_source_of_data_and_activate_it()\n",
    "            covid19_use_api = False\n",
    "            break\n",
    "        with open(\"unpolished_data/covid19/\" + AdmUnitID + \".txt\", \"w\") as file:\n",
    "            file.write(json.dumps(raw_covid19_data))\n",
    "        covid19[AdmUnitID] = dict()\n",
    "        covid19[AdmUnitID]['cases'] = [e['attributes']['KumFall'] for e in raw_covid19_data['features']]\n",
    "        \n",
    "    if covid19_use_api:\n",
    "        covid19_use_polished_data = False\n",
    "        covid19_use_api_backup = False\n",
    "        print(\"Covid19 Data directly from API is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Pull\" from Local API Backup\n",
    "If the use of the data from a local backup of the API-pull is requested and possible, the data is read from the files in the folder \"covid19\" inside the folder \"unpolished_data\". The name of the files should represent the \"AdmUnitID\" of the county.<br/>\n",
    "The data is polished and stored in the dictionary \"covid19\" during the reading progress.\n",
    "<br/><br/>\n",
    "The received data is checked: If any county has fewer timestamps than the dates stored in \"non_county_specific_data['unixtime']\", all data gets deleted to prevent the use of faulty data and an alternative data source is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_api) and covid19_use_api_backup:\n",
    "    print(\"Reading backup of old API pull...\")\n",
    "    covid19 = dict()\n",
    "    non_county_specific_data = dict()\n",
    "    list_of_countys = list(counties_geography.keys())\n",
    "    # get the dates\n",
    "    with open(\"unpolished_data/covid19/dates.txt\", \"r\") as file:\n",
    "        raw_dates = json.loads(file.read())\n",
    "    non_county_specific_data['unixtime'] = [e['attributes']['Datum'] for e in raw_dates['features']]\n",
    "    number_of_timestamps = len(non_county_specific_data['unixtime'])\n",
    "\n",
    "    for root, dirs, files in os.walk('unpolished_data/covid19'):\n",
    "        # to little dates - something is wrong. Checking here to skip for-loop\n",
    "        if len(raw_dates['features']) < 200:\n",
    "            print(\"There are only {} dates - check your backup or make a new pull from the api.\"\n",
    "                  .format(len(raw_dates['features'])))\n",
    "            find_alternative_source_of_data_and_activate_it()\n",
    "            covid19_use_api = False\n",
    "            break\n",
    "        for filename in files:\n",
    "            AdmUnitID = filename[:-4]\n",
    "            if AdmUnitID == 'dates':    # already done\n",
    "                continue\n",
    "\n",
    "            list_of_countys.remove(AdmUnitID)\n",
    "            covid19[AdmUnitID] = dict()\n",
    "            with open(os.path.join(root, filename), \"r\") as file:\n",
    "                covid19[AdmUnitID]['cases'] = [e['attributes']['KumFall'] for e in\n",
    "                                               json.loads(file.read())['features']]\n",
    "\n",
    "            if number_of_timestamps != len(covid19[AdmUnitID]['cases']):\n",
    "                print(\"The data from file {} does not have {} timestamps, it has {}.\"\n",
    "                      .format(filename, number_of_timestamps, len(covid19[AdmUnitID])))\n",
    "                find_alternative_source_of_data_and_activate_it()\n",
    "                covid19_use_api_backup = False\n",
    "                break\n",
    "\n",
    "    if len(list_of_countys) > 0 and covid19_use_api_backup:\n",
    "        print(\"No backup found for {}\".format(list_of_countys))\n",
    "        find_alternative_source_of_data_and_activate_it()\n",
    "        covid19_use_api_backup = False\n",
    "\n",
    "    if covid19_use_api_backup:\n",
    "        covid19_use_polished_data = False\n",
    "        covid19_use_api = False\n",
    "        print(\"Covid19 Data from (maybe old) API-pull-backup is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Seven Days Incidence and Get the Cases, Incidences and Inhabitants of Germany\n",
    "The calculation of the incidence needs the number of cases seven days prior (set to zero if not defined), the cases of the current day (both from \"covid19[AdmUnitID]['cases']\") and the number of inhabitants of the county (\"counties_geography[AdmUnitID]['population']\").\n",
    "<br/><br/>\n",
    "To get all new cases in that county within the last seven days the program subtracts the accumulated cases seven days earlier from the accumulated cases of the current day. Afterwards this number of cases is divided by the county's population. In order to scale it to 100,000 inhabitants, the result is multiplied by 100,000.<br/>\n",
    "This is done for every case number of every county.\n",
    "<br/><br/>\n",
    "The highest and lowest seven days incidence and the highest and lowest case number are stored in the dictionary \"non_county_specific_data\" as a reference.\n",
    "<br/><br/><br/>\n",
    "The number of inhabitants of Germany is calculated by adding the number of inhabitants of the counties. The same applies to accumulated number of COVID-19 cases for every day. The seven days incidence is calculated as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    non_county_specific_data['population_germany'] = 0\n",
    "    for county in counties_geography.values():\n",
    "        non_county_specific_data['population_germany'] += county['population']\n",
    "\n",
    "    ncsd = non_county_specific_data\n",
    "    ncsd['cases_germany'] = len(ncsd['unixtime'])*[0]\n",
    "\n",
    "    non_county_specific_data['highest_case_number'] = 0\n",
    "    non_county_specific_data['lowest_case_number'] = 100000000000000\n",
    "    non_county_specific_data['highest_incidence'] = 0\n",
    "    non_county_specific_data['lowest_incidence'] = 100000000000000\n",
    "    for AdmUnitID in covid19.keys():\n",
    "        covid19[AdmUnitID]['incidences'] = list()\n",
    "        for timestamp in range(len(covid19[AdmUnitID]['cases'])):\n",
    "            cases_7_days_prior = 0\n",
    "            cases_on_day = covid19[AdmUnitID]['cases'][timestamp]\n",
    "            non_county_specific_data['cases_germany'][timestamp] = (cases_on_day +\n",
    "            non_county_specific_data['cases_germany'][timestamp])\n",
    "\n",
    "            if timestamp >= 7:\n",
    "                cases_7_days_prior = covid19[AdmUnitID]['cases'][timestamp - 7]\n",
    "            incidence = (((cases_on_day - cases_7_days_prior) * 100000) /\n",
    "                         counties_geography[AdmUnitID]['population'])\n",
    "            covid19[AdmUnitID]['incidences'].append(incidence)\n",
    "            if non_county_specific_data['highest_case_number'] < cases_on_day:\n",
    "                non_county_specific_data['highest_case_number'] = cases_on_day\n",
    "            if non_county_specific_data['lowest_case_number'] > cases_on_day:\n",
    "                non_county_specific_data['lowest_case_number'] = cases_on_day\n",
    "            if non_county_specific_data['highest_incidence'] < incidence:\n",
    "                non_county_specific_data['highest_incidence'] = incidence\n",
    "            if non_county_specific_data['lowest_incidence'] > incidence:\n",
    "                non_county_specific_data['lowest_incidence'] = incidence\n",
    "\n",
    "    non_county_specific_data['incidences_germany'] = list()\n",
    "    for timestamp in range(len(non_county_specific_data['cases_germany'])):\n",
    "        cases_7_days_prior = 0\n",
    "        cases_on_day = non_county_specific_data['cases_germany'][timestamp]\n",
    "        if timestamp >= 7:\n",
    "            cases_7_days_prior = non_county_specific_data['cases_germany'][timestamp - 7]\n",
    "        incidence = (((cases_on_day - cases_7_days_prior) * 100000) /\n",
    "                     non_county_specific_data['population_germany'])\n",
    "        non_county_specific_data['incidences_germany'].append(incidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Names of the German Federal States\n",
    "This data is hardcoded because it is unlikely to change. Even if the names of the federal states become outdated and do not fit the current official name, the functionality of this project will not be affected.\n",
    "The names are taken from the [\"COVID-19 Datenhub\"](https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/rki_admunit_hubv/FeatureServer/0/query?where=AdmUnitId%3C20&resultType=none&outFields=*&f=pjson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    non_county_specific_data['states'] = {\n",
    "        \"1\" : \"Schleswig-Holstein\",\n",
    "        \"2\" : \"Hamburg\",\n",
    "        \"3\" : \"Niedersachsen\",\n",
    "        \"4\" : \"Bremen\",\n",
    "        \"5\" : \"Nordrhein-Westfalen\",\n",
    "        \"6\" : \"Hessen\",\n",
    "        \"7\" : \"Rheinland-Pfalz\",\n",
    "        \"8\" : \"Baden-Württemberg\",\n",
    "        \"9\" : \"Bayern\",\n",
    "        \"10\" : \"Saarland\",\n",
    "        \"11\" : \"Berlin\",\n",
    "        \"12\" : \"Brandenburg\",\n",
    "        \"13\" : \"Mecklenburg-Vorpommern\",\n",
    "        \"14\" : \"Sachsen\",\n",
    "        \"15\" : \"Sachsen-Anhalt\",\n",
    "        \"16\" : \"Thüringen\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and Save the Polished Covid19 Data\n",
    "Before the COVID-19 cases are saved in the file \"german_covid19.txt\" inside the folder \"polished_data\", they are checked once again to ensure that during the polishing nothing gets lost or is changed.\n",
    "<br/>\n",
    "It is checked if there are fewer or more counties than defined in the variable number_of_counties and if every list of cases is as long as the dedicated dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(covid19_use_polished_data):\n",
    "    covid19_data_seems_to_be_flawless = True    # Assume everything is correct\n",
    "    if len(covid19) != number_of_counties:\n",
    "        print(\"covid19 has not the right amount of counties: {} instead of {}.\"\n",
    "                .format(len(covid19), number_of_counties))\n",
    "        covid19_data_seems_to_be_flawless = False\n",
    "    for AdmUnitID in covid19.keys():\n",
    "        if len(covid19[AdmUnitID]['cases']) != len(non_county_specific_data['unixtime']):\n",
    "            print(\"The county {} has not the right amount of dates: {} instead of {}.\"\n",
    "                    .format(county, len(covid19[AdmUnitID]['cases']),\n",
    "                            len(non_county_specific_data['unixtime'])))\n",
    "            covid19_data_seems_to_be_flawless = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved seemingly flawless covid19 data.\n"
     ]
    }
   ],
   "source": [
    "if not(covid19_use_polished_data) and covid19_data_seems_to_be_flawless:\n",
    "    # check if the needed directory is availlable - otherwise create it\n",
    "    if not(os.path.isdir(\"polished_data\")): os.makedirs(\"polished_data\")\n",
    "    with open(\"polished_data/german_covid19.txt\", \"w\") as file:\n",
    "        file.write(json.dumps((covid19, non_county_specific_data)))\n",
    "    print(\"Saved seemingly flawless covid19 data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Polished Data\n",
    "If the pull from the API and/or the \"pull\" from the local backup failed or the user chose to use the polished data, the file \"german_covid19.txt\" inside the folder \"polished_data\" is opened and the data is stored in the variables \"covid19\" and \"non_county_specific_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if covid19_use_polished_data:\n",
    "    covid19_use_api_backup = False\n",
    "    covid19_use_api = False\n",
    "    with open(\"polished_data/german_covid19.txt\", \"r\") as file:\n",
    "        covid19, non_county_specific_data = json.loads(file.read())\n",
    "    print(\"Polished covid19 data from file is ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add UTC Time and Additional Dates\n",
    "Humans are generally not used to the Unix time; this is why the more accessible kind of time format UTC is chosen. The exact hour in Germany and the time shift are not taken to account because the data is only compared to other data with the same time shift.\n",
    "<br/><br/>\n",
    "The UTC time is added after saving the data because the UTC time format cannot be saved in json format. Therefore it must always be generated anew. Calculating it inside the file \"get_data.ipynb\" keeps the plotting of the data strictly separated from the pulling and polishing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_county_specific_data['UTC'] = [datetime.date.fromtimestamp(date//1000).strftime('%d.%m.%Y')\n",
    "                           for date in non_county_specific_data['unixtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_county_specific_data['UTC+7days'] = non_county_specific_data['UTC'].copy()\n",
    "for e in range(1,8):\n",
    "    non_county_specific_data['UTC+7days'].insert(0,\n",
    "    datetime.date.fromtimestamp((non_county_specific_data['unixtime'][0]\n",
    "                                - (e*86400000))//1000).strftime('%d.%m.%Y'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
